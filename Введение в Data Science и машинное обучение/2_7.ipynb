{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2_7**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2_7_1\\\n",
    "Начнём с простого - создайте Decision Tree классификатор, используя одноимённый класс из библиотеки sklearn и сохраните его в переменную dt.\n",
    "\n",
    "﻿У дерева должны быть следующие параметры:\n",
    "максимальная глубина - 5 уровней\n",
    "минимальное число образцов в вершине для разделения - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt = DecisionTreeClassifier(splitter = \"5\",max_depth = 5, min_samples_split = 5, min_samples_leaf = 2, max_features = 2) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2_7_2\\\n",
    "Теперь, создав дерево, давайте обучим его и попробуем что-нибудь предсказать!\n",
    "\n",
    "Для начала опробуем наше дерево на классическом наборе iris, где собраны данные о длине, ширине чашелистиков и лепестков ирисов и их принадлежности к виду. В sklearn он уже встроен, что довольно удобно.\n",
    "\n",
    "Итак, вам даны 2 numpy эррея с измеренными признаками ирисов и их принадлежностью к виду. Сначала попробуем примитивный способ с разбиением данных на 2 датасэта. Используйте функцию train_test_split для разделения имеющихся данных на тренировочный и тестовый наборы данных, 75% и 25% соответственно.\n",
    "Затем создайте дерево dt с параметрами по умолчанию и обучите его на тренировочных данных, а после предскажите классы, к которым принадлежат данные из тестовой выборки, сохраните результат предсказаний в переменную predicted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train)\n",
    "predicted = dt.predict(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2_7_3\\\n",
    "Одно дерево - хорошо, но где гарантии, что оно является лучшим, или хотя бы близко к нему? Одним из способов найти более-менее оптимальный набор параметров дерева является перебор множества деревьев с разными параметрами и выбор подходящего.\n",
    "Для этой цели существует класс GridSearchCV, перебирающий каждое из сочетаний параметров среди заданных для модели, обучающий её на данных и проводящих кросс-валидацию. После этого в аттрибуте .best_estimator_ храниться модель с лучшими параметрами.\n",
    "Это применимо не только к деревьям, но и к другим моделям sklearn.\n",
    "\n",
    "Теперь задание - осуществите перебор всех деревьев на данных ириса по следующим параметрам:\n",
    "максимальная глубина - от 1 до 10 уровней\n",
    "минимальное число проб для разделения - от 2 до 10\n",
    "минимальное число проб в листе - от 1 до 10\n",
    "и сохраните в переменную best_tree лучшее дерево. Переменную с GridSearchCV назовите search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "parameters = {'criterion': ['gini', 'entropy'], 'max_depth': range(1, 11), \n",
    "              'min_samples_split': range(2, 11), 'min_samples_leaf': range(1, 11)}\n",
    "clf = DecisionTreeClassifier()\n",
    "search = GridSearchCV(clf, parameters)\n",
    "search.fit(X, y)\n",
    "best_tree = search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2_7_4\\\n",
    "Чем больше данных, сложность модели и число её параметров, тем дольше будет вестись поиск GridSearchCV. Однако бывают случаи, когда модель нужна здесь и сейчас, и для этого есть RandomizedSearchCV! Пробегаясь по рандомной подвыборке параметров, он ищет наиболее хорошую модель и делает это быстрее полного перебора параметров, хотя и может пропустить оптимальные параметры.\n",
    "\n",
    "Здесь можно посмотреть на сравнение этих поисков.\n",
    "\n",
    "Осуществим поиск по тем же параметрам что и в предыдущем задании с помощью RandomizedSearchCV\n",
    "\n",
    "максимальная глубина - от 1 до 10 уровней\n",
    "минимальное число проб для разделения - от 2 до 10\n",
    "минимальное число проб в листе - от 1 до 10\n",
    "Cохраните в переменную best_tree лучшее дерево. Переменную с RandomizedSearchCV назовите search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "parameters = {'criterion': ['gini', 'entropy'], 'max_depth': range(1, 11), \n",
    "              'min_samples_split': range(2, 11), 'min_samples_leaf': range(1, 11)}\n",
    "clf = DecisionTreeClassifier()\n",
    "search = RandomizedSearchCV(clf, parameters)\n",
    "search.fit(X, y)\n",
    "best_tree = search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2_7_5\\\n",
    "Даны 2 датасэта, к которым вы можете обращаться:\n",
    "\n",
    "    train - размеченный с известными правильным ответами (хранятся в колонке y)\n",
    "    test - набор, где нужно предсказать их\n",
    "Найдите дерево с наиболее подходящими параметрами с помощью GridSearchCV и предскажите с его помощью ответы ко 2-ому сэту! Границы параметров как раньше:\n",
    "\n",
    "максимальная глубина - от 1 до 10 уровней\n",
    "минимальное число проб для разделения - от 2 до 10\n",
    "минимальное число проб в листе - от 1 до 10\n",
    "Названия переменных тоже:лучшее дерево - best_tree, GridSearchCV - search, а предсказания - predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X = train.drop([\"y\"], axis=1)\n",
    "y = train.y\n",
    "\n",
    "parameters = {'criterion': ['gini', 'entropy'], 'max_depth': range(1, 11), \n",
    "              'min_samples_split': range(2, 11), 'min_samples_leaf': range(1, 11)}\n",
    "clf = DecisionTreeClassifier()\n",
    "search = GridSearchCV(clf, parameters)\n",
    "search.fit(X, y)\n",
    "best_tree = search.best_estimator_\n",
    "\n",
    "predictions = best_tree.predict(test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2_7_6\\\n",
    "При классификации модель может допускать ошибки, присваивая наблюдению неверный класс. Существуют различные метрики оценки качества предсказаний, которые базируются на 4-ёх параметрах - true positive, false positive, false negative и true negative, соответствующих тому какой класс был присвоен наблюдениям каждого из классов. Матрицу из 4-ёх (в случае бинарной классификации) этих параметров называют confusion matrix.\n",
    "\n",
    "В sklearn можно её удобно получить с помощью функции confusion_matrix. Вам даны 2 эррея с истинными классами наблюдений и предсказанными - y и predictions. Получите по ним confusion matrix и поместите её в переменную conf_matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "conf_matrix = confusion_matrix(y, predictions)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
